#!/usr/bin/env bds


# Function to run FASTQC
string qualityControl(string{} params, string{} globalVars) {

   # If there are Trimlogs in the rawData file, PEMA does not run as it should. Hence, we remove them if there are any!
   # I re-run this command because of FastQC and the way it reacts in multiple runs over the same folders
   globalVars{'dataPath'}.chdir() ; 
   sys if [ $(find '$globalVars{'dataPath'}' -name 'TrimLog*') ] ; then  rm TrimLog* ; fi

   # make a copy of 'parameters.tsv' file to the output folder.
   # this way the user will be able to get the parameters he used for analysis, anytime!
   sys cp $globalVars{'parameterFilePath'}/parameters.tsv $globalVars{'outputFilePath'}/parameters0f.$globalVars{'analysisName'}.tsv

   # now FASTQC runs for all files
   string[] rawFiles = globalVars{'dataPath'}.dir()
   for ( string rawFile : rawFiles ) {
      println(rawFile)
      task $globalVars{'path'}/tools/fastqc/FastQC/fastqc --outdir $globalVars{'fastqcPath'}  $globalVars{'dataPath'}/$rawFile
   }
   wait
   println('FastQC has been completed!')


   # now i create a checkpoint and read again the parameters file.
   # this way if i change any of my parameters, i can do only the steps from here and after with the new ones.
   if ( globalVars{'fastqcPath'}.isEmpty() == false ) {
      string checkTrim = globalVars{'outputPoint'} + '/trimming.chp'
      checkpoint checkTrim
   }

   return 'ok'

}

# Function to run CUTADAPT
string cutadaptForIts(string{} params, string{} globalVars) {

   globalVars{'dataPath'}.chdir()

   string cutadaptOutputDirectory = 'cutadapt'
   string initialData = 'initialData'

   if ( cutadaptOutputDirectory.isDir() && initialData.isDir() ) {
      println('A directory for the cutadapt output is already there.') 

   } else {
      cutadaptOutputDirectory.mkdir()
      initialData.mkdir()
      sys chmod 777 $globalVars{'outputFilePath'} -R
      println('An output directory for cutadapt was just made.')
   }   
   
   wait
   task /usr/local/bin/Rscript $globalVars{'path'}/scripts/cutadaptITS.R $params{'forwardITSPrimer'} $params{'reverseITSPrimer'} $globalVars{'dataPath'}
   
   wait
   
   sys mv *.fastq.gz initialData
   sys mv $initialData $globalVars{'outputPoint'}/$globalVars{'analysisName'}
   sys mv cutadapt/* .
   sys rm -r cutadapt
   
   wait

   println 'Cutadapt has been completed successfully.'

   return 'ok'

}

# Function to run TRIMMOMATIC
string trimSeqs(string{} params, string{} globalVars){

   # Mmake a list with all the file names on `mydata` dir
   string[] data = globalVars{'dataPath'}.dir()
   
   # Set two variables readF and readR for the forward and reverse file of each sample  
   string readF
   string readR
   int counter = 0

   for ( string file : data ) {
      
      # Here's the trick! i need to tell which file is the forward and which is the reverese. Hence, i try to split a suffix. 
      string check = file.split('_2\.fastq\.gz')[0]

      # If the split does occure then i know that i have the second file of each sample (reverse)
      # That is because the 'file' variable will always have a '.fastq.gz' suffix.
      # Hence, if there is no split, 'file' and 'check'  variables are the same.
      if ( file == check ) {
               readF = file
               println('readF is: ' + readF)
               
               # if there is a split, then they are different
               } else {
                  readR = file
                  println('readR is: ' + readR)
            }

      wait

      if ( readF.isEmpty() == false && readR.isEmpty() == false ) {

         if ( params{'maxInfo'} == 'Yes' ) {

            println('You hace selected Max INFO')
            wait
            task /usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar $globalVars{'path'}/tools/Trimmomatic/Trimmomatic-0.38/trimmomatic-0.38.jar PE \
            -threads $params{'threadsTrimmomatic'} \
            -trimlog TrimLog $globalVars{'dataPath'}/$readF $globalVars{'dataPath'}/$readR $globalVars{'trimoPath'}/filtered_max_$readF.1P.fastq.gz \
            $globalVars{'trimoPath'}/filtered_max_$readF.1U.fastq.gz \
            $globalVars{'trimoPath'}/filtered_max_$readR.2P.fastq.gz \
            $globalVars{'trimoPath'}/filtered_max_$readR.2U.fastq.gz \
            ILLUMINACLIP:/$globalVars{'path'}/tools/Trimmomatic/Trimmomatic-0.38/adapters/$params{'adapters'}:$params{'seedMismatches'}:$params{'palindromeClipThreshold'}:$params{'simpleClipThreshold'} \
            LEADING:$params{'leading'} TRAILING:$params{'trailing'} MAXINFO:$params{'targetLength'}:$params{'strictness'} MINLEN:$params{'minlen'}
            wait
            readF = ''
            readR = ''

            } else if ( params{'maxInfo'} == 'No' ) {

                  print('You have selected no Max INFO' + '\n')

                  task /usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar $globalVars{'path'}/tools/Trimmomatic/Trimmomatic-0.38/trimmomatic-0.38.jar PE \
                  -threads $params{'threadsTrimmomatic'} \
                  -trimlog TrimLog2 $globalVars{'dataPath'}/$readF $globalVars{'dataPath'}/$readR $globalVars{'trimoPath'}/filtered_$readF.1P.fastq.gz $globalVars{'trimoPath'}/filtered_$readF.1U.fastq.gz $globalVars{'trimoPath'}/filtered_$readR.2P.fastq.gz $globalVars{'trimoPath'}/filtered_$readR.2U.fastq.gz \
                  ILLUMINACLIP:/$globalVars{'path'}/tools/Trimmomatic/Trimmomatic-0.38/adapters/$params{'adapters'}:$params{'seedMismatches'}:$params{'palindromeClipThreshold'}:$params{'simpleClipThreshold'} \
                  LEADING:$params{'leading'} TRAILING:$params{'trailing'}  MINLEN:$params{'minlen'}

                  wait

                  readF = ''
                  readR = ''
            }
      }
   }

   wait
   println('Trimmomatic  is done')


   #  Make a CHECKPOINT from which will be able to restart our analysis from the error correction sterp
   # globalVars{'path'}.chdir() ; 
   globalVars{'trimoPath'}.chdir() ;
   if ( globalVars{'trimoPath'}.isEmpty() == false ) {
      string checkCor = globalVars{'outputPoint'} + '/errorCorrection.chp'
      checkpoint checkCor
   }

   return 'ok'

}

# Function to run SPAdes - BayesHammer algo
string adjustSeqs(string{} params, string{} globalVars){

   # Make a list with all the file names that there are in the folder with Trimmomatic's
   # Output -- e.g: suffix : '2.fastq.gz.2U.fastq.gz' 
   globalVars{'trimoPath'}.chdir()

   sys if [ $(find '$globalVars{'trimoPath'}' -name '*.DS_Store*') ] ; then  rm .*[DS_]* ; fi

   string[] trimos = globalVars{'trimoPath'}.dir()
   wait
   for (string filename : trimos ) {

      # Delete all the "U.fastq.gz" files using the list we just made
      if ( filename.endsWith("U\.fastq\.gz") ) {
         filename.delete()
      }
   }

   # And then make a new list with the files that remained 
   string[] finalTrimos = globalVars{'trimoPath'}.dir()

   string readBayesF
   string readBayesR
   string newDir
   string newFile
   wait

   for ( string trimmed : finalTrimos ) {
      
      # As in Trimmomatic step, we need to know the forward and the reverse file
      string trimCheck = trimmed.split("_2\.fastq\.gz\.2P\.fastq.\gz")[0]

      if ( trimmed == trimCheck ) {
         readBayesF = trimmed
         println('this is readBayesF:' + readBayesF)
         } else {
         readBayesR = trimmed
         println('this is readBayesR:' + readBayesR)
         }

      if ( readBayesF.isEmpty() == false && readBayesR.isEmpty() == false ) {
         
         if ( params{'maxInfo'} == 'Yes' ) {
            string newName = trimCheck.split("filtered\_max\_")
            newDir = newName.substr(3,13) ;
            println('trimCheck: ' + trimCheck + '\n' + 'newDir: ' + newDir)

         } else if ( params{'maxInfo'} == 'No' ) {
            string newName = trimCheck.split("filtered_")
            newDir = newName.substr(3,13) ;
            println('trimCheck: ' + trimCheck + '\n' + 'newDir: ' + newDir)
         }
         
         # Working where the output of BayesHammer is located!
         globalVars{'bayesPath'}.chdir() ; 
         newDir.mkdir() ;
         
         # Going back to Trimo's output
         globalVars{'trimoPath'}.chdir()
         sys chmod 777 $globalVars{'outputFilePath'} -R
         wait

         # And execute the BayesHammer algorithm that it is in SPAdes
         
         sys which gzip
         sys gzip --version
         println('readBayesF: ' + readBayesF +'\n' + 'and readBayesR: ' + readBayesR)
         
         task $globalVars{'path'}/tools/SPAdes/SPAdes-3.14.0/bin/spades.py --only-error-correction --threads $params{'threadsTrimmomatic'} -o $globalVars{'bayesPath'}/$newDir -1 $readBayesF -2 $readBayesR --disable-gzip-output

         wait
      
         # Clear all variables that are used in BayesHammer step in order to be re-used
         readBayesF = ''
         readBayesR = ''
         newFile = ''
         newDir = ''
      }
   }

   println('Adjusting sequences using the BayesHammer algorithm of SPAdes has been completed.')

   #  Make a checkpoint to start our pipeline from the  merging step!
   if ( globalVars{'bayesPath'}.isEmpty() == false ) {
      string checkMerge = globalVars{'outputPoint'} + '/merging.chp'
      checkpoint checkMerge
   }

   wait

   return 'ok'

}

# Function to run PANDASEQ 
string merging(string{} params, string{} globalVars){

   globalVars{'bayesPath'}.chdir()

   sys if [ $(find '$globalVars{'bayesPath'}' -name '*.DS_Store*') ] ; then  rm .*[DS_]* ; fi
   
   # Keep in a list all files that came from the BayesHammer algo leaving aside some stats file
   # Make a list with all the folders that exist in Bayes Hammer output folder - only sample names (e.g. SRR1643855 ) 
   string[] correct = globalVars{'bayesPath'}.dir()
   correct.remove('input_dataset.yaml') ;
   correct.remove( 'params.txt' ) ;
   correct.remove('spades.log') ;
   correct.remove( 'tmp')

   # 'Create' the names of forward and reverse files each time
   string baseName
   if ( params{'maxInfo'} == 'Yes' ) {
      
      # Keep as 'baseName' the prefix of every file that depends on whether  we had maxInfo or not
      baseName = 'filtered_max_'
   } else {
      baseName = 'filtered_'
   }
   
   for ( string correctFile : correct )  {

      string uniqPath = globalVars{'bayesPath'} + '/' +  correctFile + '/' + 'corrected' ;
      # Enter to the directory of a specific sample each time in the output folder of BayesHammer - previous step
      uniqPath.chdir() ;                                                              
      
      sys /bin/gzip *.fastq
      
      # In the corrected file of each sample there are two files - one for forward, one for reverse -
      # Make two variables with them
      string forward = '_1.fastq.gz.1P.fastq.00.0_0.cor.fastq.gz'
      string reverse = '_2.fastq.gz.2P.fastq.00.0_0.cor.fastq.gz'
      string forwardFile= baseName + correctFile + forward ;
      println(forwardFile)
      string reverseFile = baseName + correctFile + reverse ;
      println(reverseFile)

      wait
         
      # Execute pandaseq algorithm for merging 
      task $globalVars{'path'}/tools/PANDAseq/bin/pandaseq -f $forwardFile -r $reverseFile -6 \
      -A $params{'pandaseqAlgorithm'} -a -B -T $params{'pandaseqThreads'} -o $params{'minoverlap'} \
      -t $params{'threshold'} $params{'pandaseqMinlen'} $params{'elimination'}  > $correctFile.merged.fastq.gz

      wait

      sys mv *.merged.fastq.gz $globalVars{'spaPath'}
      globalVars{'bayesPath'}.chdir()
   }
   wait

   # Unzip all my merged files in a new file named 'unzip'
   # Go to the folder with the output of SPAdes and make a list with its elements
   globalVars{'spaPath'}.chdir() ;
   string[] merged = globalVars{'spaPath'}.dir() ;
   string unzip = 'unzip' ;

   # Make a new directory named 'unzip'
   unzip.mkdir() ;
   sys chmod 777 $globalVars{'outputFilePath'} -R
   string unzipPath = globalVars{'spaPath'} + '/unzip'

   # And remove the element 'unzip' from the list we made
   merged.remove('unzip')

   for ( string anyFile : merged ) {
      sys cp $anyFile $unzipPath
      unzipPath.chdir() ;
      string newFile = anyFile.baseName('.gz')
      sys mv $anyFile $newFile
      globalVars{'spaPath'}.chdir()
   }

   wait

   println('Merging step by SPAdes is completed')

   # Make a checkpoint to start our pipeline from the dereplication step!
   if ( globalVars{'spaPath'}.isEmpty() == false ) {
      string checkDerep = globalVars{'outputPoint'} + '/dereplication.chp'
      checkpoint checkDerep
   }

   globalVars{'parameterFilePath'}.chdir()
   string j = 'parameters.tsv'
   string{} paramsDereplication =  readParameterFile(j)

   wait

   return 'ok'

}

# Function to dereplicate using OBITools 
string{} obiDereplicate(string{} params, string{} globalVars){

   globalVars{'unzipPath'} = globalVars{'spaPath'} + '/unzip'

   globalVars{'unzipPath'}.chdir()

   sys if [ $(find '$globalVars{'unzipPath'}' -name '*.DS_Store*') ] ; then  rm .*[DS_]* ; fi
   string[] unzips = globalVars{'unzipPath'}.dir()

   string fileForDerepl
   for ( string nodereplicate : unzips ) {
      
      # Execute OBITools
      task $globalVars{'path'}/tools/OBI/OBI-env/bin/obiuniq -m sample $nodereplicate > $globalVars{'derePath'}/dereplicate_$nodereplicate
      wait
   }
   wait
   println('All the first steps are done! clustering is about to start!')

   string checkTableStep = globalVars{'outputPoint'} + "/tableStep.chp"
   checkpoint checkTableStep 

   wait

   return globalVars

}


# Function to build directories according to user's input
string buildDirectories(string{} params, string{} globalVars){


   globalVars{'genePath'}.chdir()

   if ( params{'gene'} == 'gene_COI' ) {
      println('Marker gene under study COI.')
      string coiPath = globalVars{'genePath'} + '/' + 'gene_COI'
      coiPath.chdir()

      if ( params{'clusteringAlgo'}  == 'algo_CROP' ) {
         string algo = 'CROP'
         algo.mkdir()
      } else if ( params{'clusteringAlgo'} == 'algo_Swarm' ) {
         string algo = 'SWARM'
         algo.mkdir()
      }

   } else if ( params{'gene'} == 'gene_16S' ) {
      println('Marker gene under study 16S.')
      string sixteenPath = globalVars{'genePath'} + '/' + 'gene_16S'
      sixteenPath.chdir()
      
      if ( params{'clusteringAlgo'} == 'algo_Swarm' ) {
         string algo = 'swarm'
         algo.mkdir()
      } else if ( params{'clusteringAlgo'} == 'algo_vsearch' ) {
         string algo = 'vsearch'
         algo.mkdir()
      }

   } else if ( params{'gene'} == 'gene_18S' ) {
      println('Marker gene under study 18S.')
      string sixteenPath = globalVars{'genePath'} + '/' + 'gene_18S'
      sixteenPath.chdir()
      
      if ( params{'clusteringAlgoFor16S_18SrRNA'} == 'algo_Swarm' ) {
         string algo = 'swarm'
         algo.mkdir()
      } else if ( params{'clusteringAlgoFor16S_18SrRNA'} == 'algo_vsearch' ) {
         string algo = 'vsearch'
         algo.mkdir()
      }   

   } else if ( params{'gene'} == 'gene_ITS' ) {
      println('My marker gene is ITS.')
      string itsPath = globalVars{'genePath'} + '/' + 'gene_ITS'
      itsPath.chdir()

      if ( params{'clusteringAlgo'}  == 'algo_CROP' ) {
         string algo = 'CROP'
         algo.mkdir()
      } else if ( params{'clusteringAlgo'} == 'algo_Swarm' ) {
         string algo = 'SWARM'
         algo.mkdir()
      }      
   }
   # Again make everything 777
   sys chmod 777 $globalVars{'outputFilePath'} -R

   return 'ok'

}


# # Function to dereplicate using AWK commands and scripts
# string swarmDereplicate(string{} params, string{} globalVars){

#    globalVars{'unzipPath'}.chdir()


#    task for file in $(ls); do awk 'NR==1 {print ; next} {printf /^>/ ? "\n"$0"\n" : $1} END {printf "\n"}' $file > $file.linearized.fasta ; done


#    # task for file in $(ls | grep fasta); do  grep -v "^>" $file |grep -v [^ACGTacgt] | sort -d | uniq -c | while read abundance sequence ; do     hash=$(printf "${sequence}" | sha1sum);     hash=${hash:0:40};     printf ">%s_%d_%s\n" "${hash}" "${abundance}" "${sequence}"; done | sort -t "_" -k2,2nr -k1.2,1d | sed -e 's/\_/\n/2' > $file._dereplicated.fasta ; done

#    return 'ok'

# }







