#!/usr/bin/env bds

# This is P.E.M.A!  A metabarcoding pipenline for environmental DNA samples.
# Author: Haris Zafeiropoulos 
# date: 2017-2018
# This script is meant for the Singularity image version of P.E.M.A.

include "modules/initialize"
include "modules/preprocess"
include "modules/clustering"
include "modules/taxAssignment"
include "modules/phyloseq"


# Read parameters file for the first time! 
string parameterFilePath = "/mnt/analysis/"
parameterFilePath.chdir()
string f = "parameters.tsv"                           
string{} paramsFirstAssignment =  readParameterFile(f)


###############################################################
#                 0. Initialize analysis                      #
###############################################################
string{} globalVars = initializeAnalysis(paramsFirstAssignment)
wait

# IMPORTANT! Module to train classifiers is enabled through this statement
if ( paramsFirstAssignment{'custom_ref_db'} == 'Yes' ){
   
   string{} initGlobalVars = globalVars
   wait
   string{} globalVars = trainClassifier(paramsFirstAssignment, initGlobalVars)
   
}
wait

###############################################################
#                   1. Quality Control                        #
###############################################################

qualityControl(paramsFirstAssignment, globalVars)

################################################################
#                   2. Trimming                                #
################################################################

parameterFilePath.chdir()
string g = "parameters.tsv"
string{} paramsTrimmoStep =  readParameterFile(g)

globalVars{'dataPath'}.chdir()

# If ITS as marker gene, remove length variablity with cutadat
if ( paramsTrimmoStep{'gene'} == 'gene_ITS' ){
   cutadaptForIts(paramsTrimmoStep, globalVars)
}

# Trim fastq files using Trimmomatic
trimSeqs(paramsTrimmoStep, globalVars)
wait


################################################################
#              3. Sequence adjustment                          #
################################################################
parameterFilePath.chdir()
string h = "parameters.tsv"
string{} paramsBayesHammerStep =  readParameterFile(h)
 
adjustSeqs(paramsBayesHammerStep, globalVars)
wait


################################################################
#          4. Merging the paired-end samples                   #
################################################################

parameterFilePath.chdir()
string i = "parameters.tsv"
string{} paramsSpadesMerging = readParameterFile(i)

merging(paramsSpadesMerging, globalVars)
wait


################################################################
#          5. Dereplicate sequence samples                     #
################################################################

################################################################
#      Build subdirectories according to the params file       #
################################################################

buildDirectories(paramsSpadesMerging, globalVars)

parameterFilePath.chdir()
string j = "parameters.tsv"
string{} paramsDereplication =  readParameterFile(j)
wait

if ( paramsDereplication{'clusteringAlgo'} == 'algo_Swarm' ) {

   println('Pema is at the dereplication step; this may take a while.')
   swarmDereplicate(paramsDereplication, globalVars)


} else {

   globalVars = obiDereplicate(paramsDereplication, globalVars)

   wait

   mergeFilesNonSwarm(paramsDereplication, globalVars)

}


string clustering = globalVars{'outputPoint'} + "/clustering.chp"
checkpoint clustering

################################################################
#           6. OTUs clustering / ASVs inference                #
################################################################
 
parameterFilePath.chdir()
string k = "parameters.tsv"
string{} paramsOfTable =  readParameterFile(k)

if ( paramsDereplication{'clusteringAlgo'} == 'algo_vsearch' ) {

   clusteringVsearch(paramsOfTable, globalVars)

} else if (paramsDereplication{'clusteringAlgo'} == 'algo_Swarm') {

   clusteringSwarm(paramsOfTable, globalVars)

}

string taxAssign = globalVars{'outputPoint'} + "/taxAssign.chp"
checkpoint taxAssign

################################################################
#                7. Taxonomic assignment                       #
################################################################

parameterFilePath.chdir()
string l = "parameters.tsv"
string{} paramsForTaxAssign =  readParameterFile(l)

if ( paramsForTaxAssign{'custom_ref_db'} != 'Yes'){

   if ( paramsForTaxAssign{'gene'} == 'gene_16S' || paramsForTaxAssign{'gene'} == 'gene_18S' || paramsForTaxAssign{'gene'} == 'gene_ITS') {


      if (paramsForTaxAssign{'classifierAlgo'} == 'RDPClassifier') {

         println('Pema cannot use RDPClassifier for your marker gene. It will use CREST instead and the relative parameter will change accordingly.')

         globalVars{'classifierAlgo'} = 'CREST'

      }


      if (paramsForTaxAssign{'taxonomyAssignmentMethod'} != 'phylogeny') {

         crestAssign(paramsForTaxAssign, globalVars)

      } else {
         
         phylogenyAssign(paramsForTaxAssign, globalVars)

      }

   } else {

      if ( paramsForTaxAssign{'gene'} == 'gene_COI' ) {

         if ( paramsForTaxAssign{'classifierAlgo'} == 'CREST' ) {

            println('Pema cannot run COI data using the CREST classifier. It will use RDPClassifer instead and the relative parameter will change accordingly.')

            globalVars{'classifierAlgo'} = 'RDPClassifier'

         }

         rdpAssign(paramsForTaxAssign, globalVars)

      }
   }

} else {

   if ( paramsForTaxAssign{'classifierAlgo'} == 'RDPClassifier' ) {

      rdpAssign(paramsForTaxAssign, globalVars)

   } else if ( paramsForTaxAssign{'classifierAlgo'} == 'CREST' ) {


   } else {

      println('The classifier you chose is not available in the Pema framework.')
   }

}


string communAnal = globalVars{'outputPoint'} + "/communAnal.chp"
checkpoint communAnal

################################################################
#                8. Biodiversity analysis                      #
################################################################

parameterFilePath.chdir()
string ca = "parameters.tsv"
string{} paramsForCA =  readParameterFile(ca)


if ( paramsForCA{'phyloseq'} == 'Yes') {

   communityAnalysis(paramsForCA, globalVars)

}

wait

################################################################
#                Pema has been concluded                      #
################################################################

###  Make a file where i will move all the checkpoints
parameterFilePath.chdir() ;
if ( paramsForCA{'emptyCheckpoints'} == 'Yes' ) {
   string checkpointsDir = 'checkpoints_for_' + paramsForCA{'outputFolderName'}
   globalVars{'outputFilePath'}.chdir()
   checkpointsDir.mkdir()
   globalVars{'outputPoint'}.chdir()   
   sys mv *.chp $globalVars{'outputFilePath'}/$checkpointsDir
}


println('----------------------------------------------------------')
println('Pema has been completed successfully. Let biology start!')
println('Thanks for using Pema.')



   
# / Clustering to OTUs by making use of the CROP algorithm   ///////////
   
#    if ( paramsOfTable{'clusteringAlgoForCOI_ITS'} == 'algo_CROP' ) {
      
#       string croPath = genePath + '/' + paramsOfTable{'gene'}  + '/' + 'CROP'
      
      
#       ####   CROP    #####
#       ##  In this case we do chimera removal first and then we apply the clustering algorithm  ----  Until now (23/3) we think we prefer CROP over SWARM
      
#       outputFilePath.chdir()

#       task sed 's/_/;size=/g' final_all_samples.fasta > COI_CROP_all_samples.fasta
#       wait
#       sys cp COI_CROP_all_samples.fasta $croPath
#       wait

#       ###### we build a contingency table for AMPLICONS  - here the awk.sh file is a bash script that you can find in the PEMA file
#       linearPath.chdir()
      
#       task{
#          sys bash $path/scripts/createContigencyTable.sh
#          sys sed 's/linearized.dereplicate_//g; s/.merged.fastq//g' amplicon_contingency_table_temp.csv >  amplicon_contingency_table.csv
#          sys  mv  amplicon_contingency_table.csv  $croPath
#       }
#       wait
      
#       print 'the contigency table has been created successfully!' + "\n"

#       croPath.chdir()
      
#       ### chimera removal
#       task $path/tools/VSEARCH/vsearch-2.9.1-linux-x86_64/bin/vsearch --uchime3_denovo COI_CROP_all_samples.fasta --nonchimeras no_chim_COI_CROP_all_samples.fasta --abskew $paramsOfTable{'abskew'}
#       wait



#         ##### [NEW STEP: 2020.08.13] : Fix CROP bug. Run CROP with multiple CPUs as well


#       ### calculate parameters for CROP clustering algorithm   
#       croPath.chdir()
#       string OMP_NUM_THREADS = paramsOfTable{'omp_num_threads'}
#       sys export OMP_NUM_THREADS=$OMP_NUM_THREADS
#       sys bash $path/scripts/croParameters.sh
#       wait
      
#       # read the parameters estimated
#       string param_file = "cropParameters.tsv"
#       string{} crop_params = readParameterFile(param_file)
      
#       # run the CROP algorithm
#       sys $path/tools/CROP/CROP/CROPLinux -i no_chim_COI_CROP_all_samples.fasta -o CROP_output -z $crop_params{'zeta'} -b $crop_params{'beta'} -e $crop_params{'epsilon'} || true
      
      
      
      
#       ##### at last, we create an OTU table that comes after the CROP results. Again, we have an awk command in a bash script 
#       croPath.chdir()
#       sys mv $linearPath/amplicon_contingency_table_temp.csv $croPath
#       wait
      
#       ### perform a script that will allow us to use the CROP output as it had SWARM's format
#       sys bash $path/scripts/cropOutputTranformations.sh

#       # if necessary, remove some not that nice things 
#       sys sed -i 's/:::/:/g; s/:0:0:0:0./0./g'  amplicon_contingency_table.csv
#       sys bash $path/scripts/otuContingencyTableSwarm.sh
      
#       # The file returned is called "SWARM_OTU_table.tsv" just to have a sole script for this task for both the cases of Swarm and Crop. It is exactly the same approach.
#       sys mv SWARM_OTU_table.tsv CROP_OTU_table.tsv
#       print 'the contigency table with the found otus was just created!' + "\n"
#       print 'The clustering of the MOTUs by the CROP algorithm, has been concluded!'  + "\n"
#    }
# }

# #############################################################################################################################
# #                                                                                                                   
# #                                Taxonomy Assignment       
# #                                                                                             
# ##############################################################################################################################

# string checkTaxonomy = outputPoint + "/taxonomyAssignment.chp"
# checkpoint checkTaxonomy 
# parameterFilePath.chdir()
# string l = "parameters.tsv"
# string{} paramsOfTaxonomy =  readParameterFile(l)

# #######################################################################################################
# ##############################     Taxonomy Assignment 16S     ########################################
# #######################################################################################################




# #######################################################################################################
# ###################     Taxonomy assignment for the COI and the ITS marker genes    ###################
# #######################################################################################################


# #\\\\\\\\\\\\\\ \\\\\\\\      When CROP has been used for OTUs clustering  /////////////////////////////

# if  ( paramsOfTaxonomy{'gene'} == 'gene_COI' &&  paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_CROP' ) {
   
#    string croPath = genePath + '/' + 'gene_COI'  + '/' + 'CROP'
#    croPath.chdir()
   
#    print 'The taxonomy assignment with the CROP output has STARTED! '
   
#    ## make a file where the sequence and the abundance of otus will be present 
#    task {
#         sys cat CROP_output.cluster.fasta | grep ">" > heads.txt
#         sys sed 's/=[0-9]*/=/g' heads.txt > heads2.txt
#         sys cat CROP_output.cluster.fasta | grep -v ">"  > seqs.fasta
#         sys awk '{print $2}' CROP_output.cluster > abundancies.txt
#         sys tail -n +2 abundancies.txt > abundancies2.txt
#         sys paste heads2.txt abundancies2.txt > new_heads.txt
#         sys sed 's/\t//g' new_heads.txt > final_heads.txt
#         sys paste -d'\n' final_heads.txt seqs.fasta > crop_for_rdpclassifier.fasta
#         sys rm heads.txt heads2.txt seqs.fasta abundancies.txt abundancies2.txt new_heads.txt final_heads.txt
#    }
   
#    print 'I have made the file I need for the taxonomy assignment' + "\n"
   
#    ## run RDPClassifier 
#    croPath.chdir()
#    task java -Xmx64g -jar $path/tools/RDPTools/classifier.jar classify -t $path/tools/RDPTools/TRAIN/rRNAClassifier.properties -o taxonomy_from_rdpclassifer_COI_CROP.txt crop_for_rdpclassifier.fasta


# ##### [NEW STEP: 2020.08.14] : Build final_table.tsv for the case of COI marker gene with CROP as clustering algorithm

#    wait

#    # croPath.chdir()
#    # sys bash $path/scripts/keepOnlyPerCentSimilartityForCrop.sh

#    croPath.chdir()
#    task {
#       sys cp taxonomy_from_rdpclassifer_COI_CROP.txt tax_assign_swarm_COI.txt
#       sys cp CROP_OTU_table.tsv SWARM_OTU_table.tsv
#       sys bash $path/scripts/makeMotuTableForRDPClassifier.sh
#       sys rm *_sorted tax_assign_swarm_COI.txt SWARM_OTU_table.tsv SWARM.stats SWARM.swarms
#    }

# } else if ( paramsOfTaxonomy{'gene'} == 'gene_ITS'  &&  paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_CROP') {
   
#    string croPath = genePath + '/' + 'gene_ITS'  + '/' + 'CROP'
#    croPath.chdir()
   
#    # run a script that will make all the changes needed for the SWARM output to run with LCAClassifier
   
#    sys mv CROP_OTU_table.tsv SWARM_OTU_table.tsv
#    task bash $path/scripts/transformationsOnTable.sh

#    wait
#    sys mv SWARM_OTU_table.tsv ITS_otutab_$paramsOfTaxonomy{'taxonomyFolderName'}.txt 
#    sys mv CROP_output.cluster.fasta ITS_all_samples.otus.fa
#    sys sed -i 's/;size=[0-9]*//g' ITS_all_samples.otus.fa

   
#    task $path/tools/ncbi-blast-2.8.1+/bin/blastn -task megablast -query ITS_all_samples.otus.fa -db $path/tools/CREST/LCAClassifier/parts/flatdb/unite/unite.fasta -num_alignments 100 -outfmt 5 -out ITS_myunitemod.xml -num_threads $paramsOfTaxonomy{'vsearchThreads'}

#    wait
      
#    task $path/tools/CREST/LCAClassifier/bin/classify -c $path/tools/CREST/LCAClassifier/parts/etc/lcaclassifier.conf -d unite -t ITS_otutab_$paramsOfTaxonomy{'taxonomyFolderName'}.txt -o $paramsOfTaxonomy{'taxonomyFolderName'} ITS_myunitemod.xml
   
#    wait
   
#    string taxonomyDirectoryITS = genePath + '/'+ paramsOfTaxonomy{'gene'} + '/' + 'CROP' + '/' +paramsOfTaxonomy{'taxonomyFolderName'}
#    taxonomyDirectoryITS.chdir()
#    sys cp ITS_otutab_* final_table.tsv
# }


# 











# ##############################################################################################################################
# ####  now, i have everything i need to, so I make files per sample in which i keep only the info I consider as valuable ! ####
# ##############################################################################################################################

# bayesPath.chdir()
# string[] names = bayesPath.dir()
# outputPerSamplePath.chdir()

# # for each sample we make a folder with its name, in order to put PEMA's output concerning each of those
# for ( string sample : names ) {
#     sample.mkdir()
# }

# fastqcPath.chdir()
# # in 'samples' we make a list with all file names we find in fastqc output folder - that means for all the files
# string[] samples = fastqcPath.dir()

# # we remove all .html files from our list
# for (string file : samples ) {
#    if ( file.endsWith(".html") ) {
#       samples.remove(file)
#    }
# }

# #set variables 
# string unzipfq 
# string sampleNoZip
# fastqcPath.chdir()

# for ( string sample : samples ) {
   
#    # we unzip the output of fastqc for each file
#    task unzip $fastqcPath/$sample
#    sampleNoZip = sample.split('.zip')[0]
#    unzipfq = sample.split('_[0-9]_fastqc')[0]
#    wait
   
#    # we copy two significant output files of FastQC to the sample's output folder
#    task{
#       sys cp $fastqcPath/$sampleNoZip/Images/per_base_quality.png  $outputPerSamplePath/$unzipfq
#       sys cp $fastqcPath/$sampleNoZip/summary.txt $outputPerSamplePath/$unzipfq
#    }
#    wait
#    unzipfq = ''
# }
# wait

# ########################################################################################################
# ############      making output per sample for 16S marker gene & alignment based assignment   ##########
# #########################################################################################################

# genePath.chdir()
# if  ( ( paramsOfTaxonomy{'gene'} == 'gene_16S' || paramsOfTaxonomy{'gene'} == 'gene_18S' )  &&  paramsOfTaxonomy{'taxonomyAssignmentMethod'} == 'alignment') {
   
#    string assignmentPathRNA
   
#    if ( paramsOfTaxonomy{'clusteringAlgoFor16S_18SrRNA'} == 'algo_vsearch' ) {
#       assignmentPathRNA = genePath + '/' + paramsOfTaxonomy{'gene'} + '/' + 'vsearch' + '/'  + paramsOfTaxonomy{'taxonomyFolderName'}
#    } else {
#       assignmentPathRNA = genePath + '/' + paramsOfTaxonomy{'gene'} + '/' + 'swarm' + '/'  + paramsOfTaxonomy{'taxonomyFolderName'}
#    }
#    assignmentPathRNA.chdir()
   
#    task {
#    sys sed "s/ /''/g" Richness.tsv > Richness2.tsv 
#    sys sed "s/ /''/g" All_Cumulative.tsv > All_Cumulative2.tsv 
#    sys sed "s/ /''/g" Relative_Abundance.tsv > Relative_Abundance2.tsv 
#    }
# wait

# for ( string sample : names ) {

#    wait
#    sys awk '{print $2, $3, $sample}' Richness2.tsv > Richneness_$sample.tsv
#    sys awk '{print $2, $3, $sample}' All_Cumulative2.tsv > All_Cumulative_$sample.tsv
#    sys awk '{print $2, $3, $sample}' Relative_Abundance2.tsv > Relative_Abundance_$sample.tsv
#    sys awk '{print $sample}' All_Cumulative2.tsv > All_Cumulative_only_int_$sample.tsv
#    sys awk '{print $sample}' Relative_Abundance2.tsv > Relative_Abundance_only_int_$sample.tsv
#    wait
   
#    sys paste -d ' ' Richneness_$sample.tsv All_Cumulative_only_int_$sample.tsv Relative_Abundance_only_int_$sample.tsv > profile_$sample.tsv
#    wait
#    sys rm All_Cumulative_only_int_$sample.tsv Relative_Abundance_only_int_$sample.tsv
#    wait
#    sys mv profile_$sample.tsv Richneness_$sample.tsv  All_Cumulative_$sample.tsv  Relative_Abundance_$sample.tsv $outputPerSamplePath/$sample
# }

# sys rm Richness2.tsv All_Cumulative2.tsv   Relative_Abundance2.tsv 
# }

# ###########################################################################################
# ##################      making output per sample for COI marker gene     ##################
# ###########################################################################################

# if  ( paramsOfTaxonomy{'gene'} == 'gene_COI' &&  paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_SWARM' ){
#     #using string.readLines() i will create my otu table..
#    string swarmPath = genePath + '/' + 'gene_COI' +  '/' + 'SWARM'
#    print swarmPath + "\n"
#    swarmPath.chdir()
#    string[] allSamplesList
#    string[] allContainsOfSwarm = swarmPath.dir()
#    for ( string sample : allContainsOfSwarm ) {
#       print "sample is " + sample + "\n"
#       if ( sample.endsWith('.finalTriplesPerSampleFromSwarmClustering.txt') == true  ) {
#          string sampleId = sample.split('.finalTriplesPerSampleFromSwarmClustering.txt')[0]
#          print "sample is " + sampleId + "\n"
#          sys mv $sample $outputPerSamplePath/$sampleId
#       }
#    }
#    # here, the number of the SPECIES that are found from all samples, are computed
#    sys bash $path/scripts/uniqSpecies.sh
# }      

# if  ( paramsOfTaxonomy{'gene'} == 'gene_COI' &&  paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_CROP' ){
#    print ' EDO EINAI POU XOUME TO KENAKI MAS TO TELEUTAIO.....' +"\n"
#    string cropPath = genePath + '/' + 'gene_COI' +  '/' + 'CROP'
#    print cropPath + "\n"
#    cropPath.chdir()
#    string[] allSamplesList
#    string[] allContainsOfCrop = cropPath.dir()
#    for ( string sample : allContainsOfCrop ) {
#       print "sample is " + sample + "\n"
#       if ( sample.endsWith('.finalTriplesFromCropClustering.txt') == true  ) {
#          string sampleId = sample.split('.finalTriplesFromCropClustering.txt')[0]
#          print "sample is " + sampleId + "\n"
#          sys mv $sample $outputPerSamplePath/$sampleId
#       }
#    }
   
#    # here, the number of the SPECIES that are found from all samples, are computed
#    #using string.readLines() i will create my otu table..  
#    #string.readLines() 
# }      

# string checkPhyloseq = outputPoint + "/phyloseq.chp"
# checkpoint checkPhyloseq 
# parameterFilePath.chdir()
# string m = "parameters.tsv"
# string{} paramForPhyloseq = readParameterFile(m)





# ###  make a file where i will move all the checkpoints
# parameterFilePath.chdir() ;
# if ( paramForPhyloseq{'emptyCheckpoints'} == 'Yes' ) {
#    string checkpointsDir = 'checkpoints_for_' + paramForPhyloseq{'outputFolderName'}
#    outputFilePath.chdir()
#    checkpointsDir.mkdir()
#    outputPoint.chdir()   
#    sys mv *.chp $outputFilePath/$checkpointsDir
# }

# # # keep only one OTU-table for every analysis in the case of the 16S marker gene and the alignment based taxonomy assignment method
# # if ( paramForPhyloseq{'gene'} == '16S' &&  paramForPhyloseq{'taxonomyAssignmentMethod'} == 'alignment' ) {
# # 
# #    string algoCluster = paramForPhyloseq{'clusteringAlgoFor16S_18SrRNA'}
# #    string algo = algoCluster.split('_')[1]   
# #    string assignmentPath = genePath + '/' + "gene_16S" + '/' + algo + '/' + paramForPhyloseq{'taxonomyFolderName'}
# #    assignmentPath.chdir()
# #    sys cd 
# #    sys mv 16S_otu_table.txt 16S_otutab_$paramForPhyloseq{'taxonomyFolderName'}.txt
# # }

# outputFilePath.chdir()
# print(" \n PEMA has come to an end! Let biology start! \n ")
