#!/usr/bin/env bds

# This is P.E.M.A!  A metabarcoding pipenline for environmental DNA samples.
# Author: Haris Zafeiropoulos 
# date: 2017-2018
# This script is meant for the Singularity image version of P.E.M.A.

include "modules/initialize"
include "modules/preprocess"
include "modules/clustering"
include "modules/taxAssignment"


# Read parameters file for the first time! 
string parameterFilePath = "/mnt/analysis/"
parameterFilePath.chdir()
string f = "parameters.tsv"                           
string{} paramsFirstAssignment =  readParameterFile(f)


###############################################################
#                 0. Initialize analysis                      #
###############################################################
string{} globalVars = initializeAnalysis(paramsFirstAssignment)
wait

# IMPORTANT! Module to train classifiers is enabled through this statement
if ( paramsFirstAssignment{'custom_ref_db'} == 'Yes' ){
   
   string{} initGlobalVars = globalVars
   wait
   string{} globalVars = trainClassifier(paramsFirstAssignment, initGlobalVars)
   
}
wait

###############################################################
#                   1. Quality Control                        #
###############################################################

qualityControl(paramsFirstAssignment, globalVars)

################################################################
#                   2. Trimming                                #
################################################################

parameterFilePath.chdir()
string g = "parameters.tsv"
string{} paramsTrimmoStep =  readParameterFile(g)

globalVars{'dataPath'}.chdir()

# If ITS as marker gene, remove length variablity with cutadat
if ( paramsTrimmoStep{'gene'} == 'gene_ITS' ){
   cutadaptForIts(paramsTrimmoStep, globalVars)
}

# Trim fastq files using Trimmomatic
trimSeqs(paramsTrimmoStep, globalVars)
wait


################################################################
#              3. Sequence adjustment                          #
################################################################
parameterFilePath.chdir()
string h = "parameters.tsv"
string{} paramsBayesHammerStep =  readParameterFile(h)
 
adjustSeqs(paramsBayesHammerStep, globalVars)
wait


################################################################
#          4. Merging the paired-end samples                   #
################################################################

parameterFilePath.chdir()
string i = "parameters.tsv"
string{} paramsSpadesMerging = readParameterFile(i)

merging(paramsSpadesMerging, globalVars)
wait


################################################################
#          5. Dereplicate sequence samples                     #
################################################################

################################################################
#      Build subdirectories according to the params file       #
################################################################

buildDirectories(paramsSpadesMerging, globalVars)

parameterFilePath.chdir()
string j = "parameters.tsv"
string{} paramsDereplication =  readParameterFile(j)
wait

if ( paramsDereplication{'clusteringAlgo'} == 'algo_Swarm' ) {

   println('Pema is at the dereplication step; this may take a while.')
   swarmDereplicate(paramsDereplication, globalVars)


} else {

   globalVars = obiDereplicate(paramsDereplication, globalVars)

   wait

   mergeFilesNonSwarm(paramsDereplication, globalVars)

}


string clustering = globalVars{'outputPoint'} + "/clustering.chp"
checkpoint clustering

################################################################
#           6. OTUs clustering / ASVs inference                #
################################################################
 
parameterFilePath.chdir()
string k = "parameters.tsv"
string{} paramsOfTable =  readParameterFile(k)

if ( paramsDereplication{'clusteringAlgo'} == 'algo_vsearch' ) {

   clusteringVsearch(paramsOfTable, globalVars)

} else if (paramsDereplication{'clusteringAlgo'} == 'algo_Swarm') {

   clusteringSwarm(paramsOfTable, globalVars)

}

string taxAssign = globalVars{'outputPoint'} + "/taxAssign.chp"
checkpoint taxAssign

################################################################
#                7. Taxonomic assignment                       #
################################################################

parameterFilePath.chdir()
string l = "parameters.tsv"
string{} paramsForTaxAssign =  readParameterFile(l)

if ( paramsForTaxAssign{'custom_ref_db'} != 'Yes'){

   if ( paramsForTaxAssign{'gene'} == 'gene_16S' || paramsForTaxAssign{'gene'} == 'gene_18S' || paramsForTaxAssign{'gene'} == 'gene_ITS') {

      if (paramsForTaxAssign{'taxonomyAssignmentMethod'} != 'phylogeny') {

         crestAssign(paramsForTaxAssign, globalVars)

      } else {
         
         phylogenyAssign(paramsForTaxAssign, globalVars)

      }

   } else {

      if ( paramsForTaxAssign{'gene'} == 'gene_COI' ) {

         rdpAssign(paramsForTaxAssign, globalVars)

      }
   }

} else {

   if ( paramsForTaxAssign{'classifierAlgo'} == 'RDPClassifier' ) {

      rdpAssign(paramsForTaxAssign, globalVars)

   } else if ( paramsForTaxAssign{'classifierAlgo'} == 'CREST' ) {


   } else {

      println('The classifier you chose is not available in the Pema framework.')
   }

}


println('Pema has been completed successfully. Enjoy your results! :) ')











   
   
#    ## in this case, there is not an algo that does both chimera removal and clustering, hence we have a 2 - step procedure
#    ## an kai stin arxi eixame pei prota chimera removal kai meta otu clustering, telika pame antistrofa

#    #//////////////////////////////////////////////////////////////////////////////////////////////////////////////////   
#    ###########################  ASVs inference by making use of the  SWARM algorithm   ###############################
#    #//////////////////////////////////////////////////////////////////////////////////////////////////////////////////
   
#    if ( paramsOfTable{'clusteringAlgoForCOI_ITS'} == 'algo_SWARM' ) {
      
#       #### Prepare files
#       outputFilePath.chdir()
#       string swarmPath = genePath + '/' + gene + '/' + 'SWARM'
#       string allFiles = geneName + "_SWARM_all_samples.fasta"
   
#       sys cp final_all_samples.fasta $swarmPath/$allFiles
#       wait
         
#       ###### we build a contingency table for AMPLICONS  - here the awk.sh file is a bash script that you can find in the PEMA file
#       linearPath.chdir()
      
#       task{
#          sys bash $path/scripts/createContigencyTable.sh
#          sys sed 's/linearized.dereplicate_//g; s/.merged.fastq//g' amplicon_contingency_table_temp.csv > amplicon_contingency_table.csv
#          sys mv amplicon_contingency_table.csv $swarmPath
#       }
#       wait
      
#       print 'the contigency table has been created successfully!' + "\n"
      
#       ##### Run SWARM and then chimera removal
#       swarmPath.chdir()
#       string inputFileForSwarm = geneName + "_SWARM_all_samples.fasta"
#       wait
      
      
      

#       ##### [NEW STEP: 2020.05.24] : Swarm expects dereplicated fasta files. We need to dereplicate the unified sequences file.
#       task $path/tools/swarm/swarm/bin/swarm -d 0 -t $paramsOfTable{'swarmThreads'} -w derep.fasta $inputFileForSwarm
#       wait

#       task{   
#          sys rm $inputFileForSwarm
#          sys mv derep.fasta $inputFileForSwarm
#       }      
      
#       wait      
      
      
      
#       ## here is the clustering step with SWARM algorithm    
#       if ( paramsOfTable{'d'} == '1' ) {
#          task $path/tools/swarm/swarm/bin/swarm -d 1 -f -t $paramsOfTable{'swarmThreads'} -s SWARM.stats -w SWARM_OTU_representatives_all_samples.fasta < $inputFileForSwarm > SWARM.swarms      
#       } else {
#          task $path/tools/swarm/swarm/bin/swarm -d $paramsOfTable{'d'} -t $paramsOfTable{'swarmThreads'} -s SWARM.stats -w SWARM_OTU_representatives_all_samples.fasta < $inputFileForSwarm > SWARM.swarms
#       }
#       wait
#       print('the swarm algorithm worked just fine!' + "\n")
      
#       # and now i handle its output in order to take it in the form i want to. turn lowcase to uppercase and substitute the '_' to 'size='
#       task {
#            sys cat SWARM_OTU_representatives_all_samples.fasta | tr '[a-z]' '[A-Z]' > upper.SWARM_OTU_representatives_all_samples.fasta
#            sys sed 's/_/;size=/g' upper.SWARM_OTU_representatives_all_samples.fasta > true.SWARM_OTU_representatives_all_samples.fasta
#       }
#       wait
      
#       ## 'sortbysize' is an algorithm for sorting by size (abundance)
#       task $path/tools/VSEARCH/vsearch-2.9.1-linux-x86_64/bin/vsearch --sortbysize true.SWARM_OTU_representatives_all_samples.fasta --output SWARM_final_OTU_representative.fasta
#       wait
#       print('sortbysize algorithm ran ok!' + "\n")
      
#       ## and the 'uchime3_denovo' is an algorithm for finding chimeras
#       task $path/tools/VSEARCH/vsearch-2.9.1-linux-x86_64/bin/vsearch --uchime3_denovo SWARM_final_OTU_representative.fasta --nonchimeras SWARM_otu_no_chimera.fasta --abskew $paramsOfTable{'abskew'}
#       wait
#       print('the uchime3_denovo algorithm for finding chimeras just finished!' + "\n")

#       string[] swarmInterFIles = swarmPath.dir()
#       for (string file : swarmInterFIles ) {
#          if ( file.startsWith('upper') ) {
#             file.delete()
#          }
#          if  ( file.startsWith('true') ) {
#             file.delete()
#          }
#       }      
      
#       ##### at last, we create an OTU table that comes after the SWARM results. Again, we have an awk command in a bash script 
#       swarmPath.chdir()
#       sys mv $linearPath/amplicon_contingency_table_temp.csv $swarmPath
#       wait
#       # sys bash $path/scripts/createOtuContigencyTable.sh

#       # if necessary, remove some not that nice things 
#       sys sed -i 's/:::/:/g; s/:0:0:0:0./0./g'  amplicon_contingency_table.csv
#       sys bash $path/scripts/otuContingencyTableSwarm.sh
#       print('the contigency table with the found otus was just created!' + "\n")
#    }
#    #### here ends ths "if" statement of SWARM clustering   (   ATTENTION TO HAVE 1 TAB HERE !! )

#    #//////////////////////////////////////////////////////////////////////////////////////////////////////////////////
#    ##########################   Clustering to OTUs by making use of the CROP algorithm   #############################
#    #//////////////////////////////////////////////////////////////////////////////////////////////////////////////////
   
#    if ( paramsOfTable{'clusteringAlgoForCOI_ITS'} == 'algo_CROP' ) {
      
#       string croPath = genePath + '/' + paramsOfTable{'gene'}  + '/' + 'CROP'
      
      
#       ####   CROP    #####
#       ##  In this case we do chimera removal first and then we apply the clustering algorithm  ----  Until now (23/3) we think we prefer CROP over SWARM
      
#       outputFilePath.chdir()

#       task sed 's/_/;size=/g' final_all_samples.fasta > COI_CROP_all_samples.fasta
#       wait
#       sys cp COI_CROP_all_samples.fasta $croPath
#       wait

#       ###### we build a contingency table for AMPLICONS  - here the awk.sh file is a bash script that you can find in the PEMA file
#       linearPath.chdir()
      
#       task{
#          sys bash $path/scripts/createContigencyTable.sh
#          sys sed 's/linearized.dereplicate_//g; s/.merged.fastq//g' amplicon_contingency_table_temp.csv >  amplicon_contingency_table.csv
#          sys  mv  amplicon_contingency_table.csv  $croPath
#       }
#       wait
      
#       print 'the contigency table has been created successfully!' + "\n"

#       croPath.chdir()
      
#       ### chimera removal
#       task $path/tools/VSEARCH/vsearch-2.9.1-linux-x86_64/bin/vsearch --uchime3_denovo COI_CROP_all_samples.fasta --nonchimeras no_chim_COI_CROP_all_samples.fasta --abskew $paramsOfTable{'abskew'}
#       wait



#         ##### [NEW STEP: 2020.08.13] : Fix CROP bug. Run CROP with multiple CPUs as well


#       ### calculate parameters for CROP clustering algorithm   
#       croPath.chdir()
#       string OMP_NUM_THREADS = paramsOfTable{'omp_num_threads'}
#       sys export OMP_NUM_THREADS=$OMP_NUM_THREADS
#       sys bash $path/scripts/croParameters.sh
#       wait
      
#       # read the parameters estimated
#       string param_file = "cropParameters.tsv"
#       string{} crop_params = readParameterFile(param_file)
      
#       # run the CROP algorithm
#       sys $path/tools/CROP/CROP/CROPLinux -i no_chim_COI_CROP_all_samples.fasta -o CROP_output -z $crop_params{'zeta'} -b $crop_params{'beta'} -e $crop_params{'epsilon'} || true
      
      
      
      
#       ##### at last, we create an OTU table that comes after the CROP results. Again, we have an awk command in a bash script 
#       croPath.chdir()
#       sys mv $linearPath/amplicon_contingency_table_temp.csv $croPath
#       wait
      
#       ### perform a script that will allow us to use the CROP output as it had SWARM's format
#       sys bash $path/scripts/cropOutputTranformations.sh

#       # if necessary, remove some not that nice things 
#       sys sed -i 's/:::/:/g; s/:0:0:0:0./0./g'  amplicon_contingency_table.csv
#       sys bash $path/scripts/otuContingencyTableSwarm.sh
      
#       # The file returned is called "SWARM_OTU_table.tsv" just to have a sole script for this task for both the cases of Swarm and Crop. It is exactly the same approach.
#       sys mv SWARM_OTU_table.tsv CROP_OTU_table.tsv
#       print 'the contigency table with the found otus was just created!' + "\n"
#       print 'The clustering of the MOTUs by the CROP algorithm, has been concluded!'  + "\n"
#    }
# }

# #############################################################################################################################
# #                                                                                                                   
# #                                Taxonomy Assignment       
# #                                                                                             
# ##############################################################################################################################

# string checkTaxonomy = outputPoint + "/taxonomyAssignment.chp"
# checkpoint checkTaxonomy 
# parameterFilePath.chdir()
# string l = "parameters.tsv"
# string{} paramsOfTaxonomy =  readParameterFile(l)

# #######################################################################################################
# ##############################     Taxonomy Assignment 16S     ########################################
# #######################################################################################################




# #######################################################################################################
# ###################     Taxonomy assignment for the COI and the ITS marker genes    ###################
# #######################################################################################################


# #\\\\\\\\\\\\\\ \\\\\\\\      When SWARM has been used for ASVs inferring  /////////////////////////////

      
# ###   and now the big "IF" for the case of ITS
# ###   we will use the CREST LCAClassifier for that.

#    } else if ( paramsOfTaxonomy{'gene'} == "gene_ITS" ) {
         
#       swarmPath.chdir()
      
#       # run a script that will make all the changes needed for the SWARM output to run with LCAClassifier
#       task bash $path/scripts/transformationsOnTable.sh
      
#       wait

#       sys mv SWARM_OTU_table.tsv ITS_otutab_$paramsOfTaxonomy{'taxonomyFolderName'}.txt 
#       sys mv SWARM_otu_no_chimera.fasta ITS_all_samples.otus.fa
#       sys sed -i 's/;size=[0-9]*//g' ITS_all_samples.otus.fa

      
#       task $path/tools/ncbi-blast-2.8.1+/bin/blastn -task megablast -query ITS_all_samples.otus.fa -db $path/tools/CREST/LCAClassifier/parts/flatdb/unite/unite.fasta -num_alignments 100 -outfmt 5 -out ITS_myunitemod.xml -num_threads $paramsOfTaxonomy{'vsearchThreads'}

#       wait
         
#       task $path/tools/CREST/LCAClassifier/bin/classify -c $path/tools/CREST/LCAClassifier/parts/etc/lcaclassifier.conf -d unite -t ITS_otutab_$paramsOfTaxonomy{'taxonomyFolderName'}.txt -o $paramsOfTaxonomy{'taxonomyFolderName'} ITS_myunitemod.xml
      
#       wait
      
#       string taxonomyDirectoryITS = genePath + '/' + paramsOfTaxonomy{'gene'} +  '/' + 'SWARM' + '/' + paramsOfTaxonomy{'taxonomyFolderName'}
#       taxonomyDirectoryITS.chdir()
#       sys cp ITS_otutab_* final_table.tsv
#    }      
# }

# wait

# #\\\\\\\\\\\\\\ \\\\\\\\      When CROP has been used for OTUs clustering  /////////////////////////////

# if  ( paramsOfTaxonomy{'gene'} == 'gene_COI' &&  paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_CROP' ) {
   
#    string croPath = genePath + '/' + 'gene_COI'  + '/' + 'CROP'
#    croPath.chdir()
   
#    print 'The taxonomy assignment with the CROP output has STARTED! '
   
#    ## make a file where the sequence and the abundance of otus will be present 
#    task {
#         sys cat CROP_output.cluster.fasta | grep ">" > heads.txt
#         sys sed 's/=[0-9]*/=/g' heads.txt > heads2.txt
#         sys cat CROP_output.cluster.fasta | grep -v ">"  > seqs.fasta
#         sys awk '{print $2}' CROP_output.cluster > abundancies.txt
#         sys tail -n +2 abundancies.txt > abundancies2.txt
#         sys paste heads2.txt abundancies2.txt > new_heads.txt
#         sys sed 's/\t//g' new_heads.txt > final_heads.txt
#         sys paste -d'\n' final_heads.txt seqs.fasta > crop_for_rdpclassifier.fasta
#         sys rm heads.txt heads2.txt seqs.fasta abundancies.txt abundancies2.txt new_heads.txt final_heads.txt
#    }
   
#    print 'I have made the file I need for the taxonomy assignment' + "\n"
   
#    ## run RDPClassifier 
#    croPath.chdir()
#    task java -Xmx64g -jar $path/tools/RDPTools/classifier.jar classify -t $path/tools/RDPTools/TRAIN/rRNAClassifier.properties -o taxonomy_from_rdpclassifer_COI_CROP.txt crop_for_rdpclassifier.fasta


# ##### [NEW STEP: 2020.08.14] : Build final_table.tsv for the case of COI marker gene with CROP as clustering algorithm

#    wait

#    # croPath.chdir()
#    # sys bash $path/scripts/keepOnlyPerCentSimilartityForCrop.sh

#    croPath.chdir()
#    task {
#       sys cp taxonomy_from_rdpclassifer_COI_CROP.txt tax_assign_swarm_COI.txt
#       sys cp CROP_OTU_table.tsv SWARM_OTU_table.tsv
#       sys bash $path/scripts/makeMotuTableForRDPClassifier.sh
#       sys rm *_sorted tax_assign_swarm_COI.txt SWARM_OTU_table.tsv SWARM.stats SWARM.swarms
#    }

# } else if ( paramsOfTaxonomy{'gene'} == 'gene_ITS'  &&  paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_CROP') {
   
#    string croPath = genePath + '/' + 'gene_ITS'  + '/' + 'CROP'
#    croPath.chdir()
   
#    # run a script that will make all the changes needed for the SWARM output to run with LCAClassifier
   
#    sys mv CROP_OTU_table.tsv SWARM_OTU_table.tsv
#    task bash $path/scripts/transformationsOnTable.sh

#    wait
#    sys mv SWARM_OTU_table.tsv ITS_otutab_$paramsOfTaxonomy{'taxonomyFolderName'}.txt 
#    sys mv CROP_output.cluster.fasta ITS_all_samples.otus.fa
#    sys sed -i 's/;size=[0-9]*//g' ITS_all_samples.otus.fa

   
#    task $path/tools/ncbi-blast-2.8.1+/bin/blastn -task megablast -query ITS_all_samples.otus.fa -db $path/tools/CREST/LCAClassifier/parts/flatdb/unite/unite.fasta -num_alignments 100 -outfmt 5 -out ITS_myunitemod.xml -num_threads $paramsOfTaxonomy{'vsearchThreads'}

#    wait
      
#    task $path/tools/CREST/LCAClassifier/bin/classify -c $path/tools/CREST/LCAClassifier/parts/etc/lcaclassifier.conf -d unite -t ITS_otutab_$paramsOfTaxonomy{'taxonomyFolderName'}.txt -o $paramsOfTaxonomy{'taxonomyFolderName'} ITS_myunitemod.xml
   
#    wait
   
#    string taxonomyDirectoryITS = genePath + '/'+ paramsOfTaxonomy{'gene'} + '/' + 'CROP' + '/' +paramsOfTaxonomy{'taxonomyFolderName'}
#    taxonomyDirectoryITS.chdir()
#    sys cp ITS_otutab_* final_table.tsv
# }


# ########################################################################################################################
# #############    here starts the 'makeOtuTable' script    ...    take as much information as possible   ################
# ########################################################################################################################

# string fileFromRdpClassifier
# string textFinal
# string perSampleTriples

# # make as variables the files that will be used and created, depending on the clustering algo
# if (  paramsOfTaxonomy{'gene'} == 'gene_COI'  && paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_SWARM' ) {
#    string swarmPath = genePath + '/' + 'gene_COI' +  '/' + 'SWARM'
#    fileFromRdpClassifier = 'tax_assign_swarm_COI.txt'
#    textFinal = 'finalTriplesFromSwarmClustering.txt'
#    perSampleTriples = 'finalTriplesPerSampleFromSwarmClustering.txt'
#    swarmPath.chdir()
# } else if ( paramsOfTaxonomy{'gene'} == 'gene_COI'  && paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_CROP' ) {
#    string croPath = genePath + '/' + 'gene_COI' +  '/' + 'CROP'
#    fileFromRdpClassifier = 'taxonomy_from_rdpclassifer_COI_CROP.txt'
#    textFinal = 'finalTriplesFromCropClustering.txt'
#    perSampleTriples = 'finalTriplesPerSampleFromCropClustering.txt'
#    croPath.chdir()
# }
# wait

# # we check whether the output of the classifier is non-zero
# if (  paramsOfTaxonomy{'gene'} == 'gene_COI'  && fileFromRdpClassifier.size() != 0 ) {
#    print ' we have a RDPClassifier output file to work with!!! '
#    bool debugCode = false
   
#    string{} giveSampleAndTaxonomyGetSizeSum
#    string{} sampleIdentifiers          #set of all samples encountered
#    string{} taxaNames               #set of all assigned taxa
   
# #####   from output file from both SWARM and CROP clustering  algorithms and RDPClassifier afterwards,
# #####   I get an 'assignment.txt' file, from which  i get all the information i need   
# #####   here, i keep only the taxonomy that is from a threshold and above ( e.g >97%) and i merge all otus
# #####   with the same taxonomy in the same sample in one entry
# #####   finally, i sum up the sizes of the otus i merged
   
#    string[] lines = fileFromRdpClassifier.readLines()
#    for ( string line : lines ) {
#    # input file looks like:
#    #ERR1308250:22628;size=4349_TAB__TAB_root_TAB_root_TAB_1Eukaryota   superkingdom   1   Arthropoda   phylum...
#    #0   1   2   3   4   5   6   7   8   9   10...
#    #PARSE   SKIP   SKIP   SKIP   PARSE   PARSE   SKIP   PARSE   PARSE   SKIP...
   
#       print line + "\n"
#       string[] lineElements = line.split("\t")
#       string sample = lineElements[0].split(':')[0]                                 #parse 1st column (index 0), get first component
#       string sizeAsString = lineElements[0].split('=')[1]                              #parse 1st column (index 0), get last component
#       int size = sizeAsString.parseInt()

#       if ( debugCode ) {
#          print "TEST1\t "+ sample + "\t" + size + "\n"
#       }

#       string thereIsMoreToRead = 'true'

#       #current triplet refers to e.g. "Eukaryota   superkingdom   1" columns with index 1,2,3
#       int currentTripletStartColumn = 1                                                 #start parsing now from the 2nd column, ie. with index 1
#       string assigned_taxo=""
#       string assignmentConfidence=0

#       while ( thereIsMoreToRead == 'true' ) {
         
#          assigned_taxo = lineElements[currentTripletStartColumn]                              #read column with index 5, 8, 11...
         
#          if ( debugCode ) {
#             print "t1:" + currentTripletStartColumn + ":" + assigned_taxo + "\n"
#          }

#          #columns with index 6, 9, 12,... are skipped
#          #read column with index 7, 10, 13...
#          assignmentConfidence = lineElements[currentTripletStartColumn + 2]
#          print "the assigned taxo is " + assigned_taxo + "\n"
#          print "assignmentConfidence is " + assignmentConfidence + "\n"
#          print "size is equal to: " + size + "\n"
#          if ( debugCode ) {
#             print "t2:" + currentTripletStartColumn +":" +assignmentConfidence + "\n"
#          }
#          #if no next taxon triple follows return 
#          int nextTripletConfidenceColumn = currentTripletStartColumn + 5 
#          if ( debugCode ) {
#             print "t3:" + currentTripletStartColumn + " index out of max "+ lineElements.size() - 1 +"\n"
#          }
#          #inspect column with index 8, 11, 14... for existence
#          if ( nextTripletConfidenceColumn > lineElements.size() ) { 
#             #the whole table has been read and the species level reached
#             if ( debugCode ) {
#                print "t4: column with index " + nextTripletConfidenceColumn + " does not exist\n"
#             }
#             thereIsMoreToRead = 'false'
#             continue;
#          }
#          #if the confience of the next taxon is below confidence return
#          if ( lineElements[nextTripletConfidenceColumn] < 0.97 ) {
#             if ( debugCode ) {
#                print "t5:" + nextTripletConfidenceColumn + "\n"
#             }
#             thereIsMoreToRead = 'false'
#             continue;
#          }

#          currentTripletStartColumn = currentTripletStartColumn + 3 #move to next taxon triple

#       } #end of while to read taxa and confidence levels
      
#       if ( debugCode ) {
#          print "TEST2\t "+ sample + "\t" + assigned_taxo + "\t" + assignmentConfidence + "\t" + size + "\n"
#       }
   
#    #######################     till here, my code finds the taxon level   ####################################
         
#       sampleIdentifiers { sample } = 1;
#       taxaNames { assigned_taxo } = 1;
      
#       wait
         
#       #sum size to any previous size sum corresponding to SampleAndTaxonomy pair
#       if ( giveSampleAndTaxonomyGetSizeSum.hasKey(sample+'_SEP_'+assigned_taxo ) == 'true') {
#          int newSum = giveSampleAndTaxonomyGetSizeSum { sample+'_SEP_'+assigned_taxo }.parseInt()
#          print("size is " + size)
#          newSum += size
#          print("new sum is " + newSum)
#          giveSampleAndTaxonomyGetSizeSum { sample+'_SEP_'+assigned_taxo } = newSum
#       } else {
#          giveSampleAndTaxonomyGetSizeSum { sample+'_SEP_'+assigned_taxo } =  size
#       }
      
#       if ( debugCode ) {
#          print "TEST3\t"+ sample+'_'+assigned_taxo + "-" + giveSampleAndTaxonomyGetSizeSum{ sample+'_'+assigned_taxo }  +"\n"
#       }
#    } #end of each input file line
   
#    wait
   
#    if ( debugCode ) {
#       print giveSampleAndTaxonomyGetSizeSum
#    }
   
#    #print output based on the giveSampleAndTaxonomyGetSizeSum map
#    string sampleToPrint
#    string sizeSumToPrint
#    string taxonomyToPrint
#    wait
#    for(string sampleAndTaxon : giveSampleAndTaxonomyGetSizeSum.keys() ) {
#       sampleToPrint =  sampleAndTaxon.split('_SEP_')[0]
#       sizeSumToPrint =  giveSampleAndTaxonomyGetSizeSum{ sampleAndTaxon }
#       taxonomyToPrint =  sampleAndTaxon.split('_SEP_')[1]
#       sys echo '$sampleToPrint    $sizeSumToPrint    $taxonomyToPrint ' >>  $textFinal
      
#    }
   
#    if ( debugCode ) {
#       print taxaNames
#       print sampleIdentifiers
#    }
   
#    ## double for-loop with all the sample id and taxa name possible combinations printing the real ones,
#    ## the ones that have a value that is
#    string sizeToPrintPerSample
#    for (string sampleId : sampleIdentifiers.keys()) {
#       for ( string currentTaxon : taxaNames.keys() ){
         
#          ## print only for the sample and taxonomy pairs that have a size
#          if ( giveSampleAndTaxonomyGetSizeSum.hasKey( sampleId+"_SEP_"+currentTaxon)) {
#             sizeToPrintPerSample = giveSampleAndTaxonomyGetSizeSum { sampleId+"_SEP_"+currentTaxon }
#             sys echo '$sampleId   $sizeToPrintPerSample   $currentTaxon' >>  $sampleId.$perSampleTriples
#          }
#       }
#    }
#    print 'i did just find with the tripltes! ' + "\n"
#    wait
   
#    # finaly, make an OTU table only with the significant assignments (>97%)
#    if ( paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_SWARM' ) {
#       string swarmPath = genePath + '/' + 'gene_COI' +  '/' + 'SWARM'
#       swarmPath.chdir()
#       sys bash $path/scripts/createOtuTableOnlyForTaxonomyAssigned.sh      
#    }
   
#    if ( paramsOfTaxonomy {'clusteringAlgoForCOI_ITS'} == 'algo_CROP') {
#       string cropPath = genePath + '/' + 'gene_COI' +  '/' + 'CROP'
#       cropPath.chdir()
#       sys bash $path/scripts/createOtuTableOnlyForTaxonomyAssigned.sh
#    }

# }

# print ' At last! We now have the MOTU-table! :) ' + "\n"
# wait
# ###################################     here ends 'make0tuTable' script  !!!    ###############################################

# #////////////////////////////////////////////////////////////////////////////////////////////////////////










# ##############################################################################################################################
# ####  now, i have everything i need to, so I make files per sample in which i keep only the info I consider as valuable ! ####
# ##############################################################################################################################

# bayesPath.chdir()
# string[] names = bayesPath.dir()
# outputPerSamplePath.chdir()

# # for each sample we make a folder with its name, in order to put PEMA's output concerning each of those
# for ( string sample : names ) {
#     sample.mkdir()
# }

# fastqcPath.chdir()
# # in 'samples' we make a list with all file names we find in fastqc output folder - that means for all the files
# string[] samples = fastqcPath.dir()

# # we remove all .html files from our list
# for (string file : samples ) {
#    if ( file.endsWith(".html") ) {
#       samples.remove(file)
#    }
# }

# #set variables 
# string unzipfq 
# string sampleNoZip
# fastqcPath.chdir()

# for ( string sample : samples ) {
   
#    # we unzip the output of fastqc for each file
#    task unzip $fastqcPath/$sample
#    sampleNoZip = sample.split('.zip')[0]
#    unzipfq = sample.split('_[0-9]_fastqc')[0]
#    wait
   
#    # we copy two significant output files of FastQC to the sample's output folder
#    task{
#       sys cp $fastqcPath/$sampleNoZip/Images/per_base_quality.png  $outputPerSamplePath/$unzipfq
#       sys cp $fastqcPath/$sampleNoZip/summary.txt $outputPerSamplePath/$unzipfq
#    }
#    wait
#    unzipfq = ''
# }
# wait

# ########################################################################################################
# ############      making output per sample for 16S marker gene & alignment based assignment   ##########
# #########################################################################################################

# genePath.chdir()
# if  ( ( paramsOfTaxonomy{'gene'} == 'gene_16S' || paramsOfTaxonomy{'gene'} == 'gene_18S' )  &&  paramsOfTaxonomy{'taxonomyAssignmentMethod'} == 'alignment') {
   
#    string assignmentPathRNA
   
#    if ( paramsOfTaxonomy{'clusteringAlgoFor16S_18SrRNA'} == 'algo_vsearch' ) {
#       assignmentPathRNA = genePath + '/' + paramsOfTaxonomy{'gene'} + '/' + 'vsearch' + '/'  + paramsOfTaxonomy{'taxonomyFolderName'}
#    } else {
#       assignmentPathRNA = genePath + '/' + paramsOfTaxonomy{'gene'} + '/' + 'swarm' + '/'  + paramsOfTaxonomy{'taxonomyFolderName'}
#    }
#    assignmentPathRNA.chdir()
   
#    task {
#    sys sed "s/ /''/g" Richness.tsv > Richness2.tsv 
#    sys sed "s/ /''/g" All_Cumulative.tsv > All_Cumulative2.tsv 
#    sys sed "s/ /''/g" Relative_Abundance.tsv > Relative_Abundance2.tsv 
#    }
# wait

# for ( string sample : names ) {

#    wait
#    sys awk '{print $2, $3, $sample}' Richness2.tsv > Richneness_$sample.tsv
#    sys awk '{print $2, $3, $sample}' All_Cumulative2.tsv > All_Cumulative_$sample.tsv
#    sys awk '{print $2, $3, $sample}' Relative_Abundance2.tsv > Relative_Abundance_$sample.tsv
#    sys awk '{print $sample}' All_Cumulative2.tsv > All_Cumulative_only_int_$sample.tsv
#    sys awk '{print $sample}' Relative_Abundance2.tsv > Relative_Abundance_only_int_$sample.tsv
#    wait
   
#    sys paste -d ' ' Richneness_$sample.tsv All_Cumulative_only_int_$sample.tsv Relative_Abundance_only_int_$sample.tsv > profile_$sample.tsv
#    wait
#    sys rm All_Cumulative_only_int_$sample.tsv Relative_Abundance_only_int_$sample.tsv
#    wait
#    sys mv profile_$sample.tsv Richneness_$sample.tsv  All_Cumulative_$sample.tsv  Relative_Abundance_$sample.tsv $outputPerSamplePath/$sample
# }

# sys rm Richness2.tsv All_Cumulative2.tsv   Relative_Abundance2.tsv 
# }

# ###########################################################################################
# ##################      making output per sample for COI marker gene     ##################
# ###########################################################################################

# if  ( paramsOfTaxonomy{'gene'} == 'gene_COI' &&  paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_SWARM' ){
#     #using string.readLines() i will create my otu table..
#    string swarmPath = genePath + '/' + 'gene_COI' +  '/' + 'SWARM'
#    print swarmPath + "\n"
#    swarmPath.chdir()
#    string[] allSamplesList
#    string[] allContainsOfSwarm = swarmPath.dir()
#    for ( string sample : allContainsOfSwarm ) {
#       print "sample is " + sample + "\n"
#       if ( sample.endsWith('.finalTriplesPerSampleFromSwarmClustering.txt') == true  ) {
#          string sampleId = sample.split('.finalTriplesPerSampleFromSwarmClustering.txt')[0]
#          print "sample is " + sampleId + "\n"
#          sys mv $sample $outputPerSamplePath/$sampleId
#       }
#    }
#    # here, the number of the SPECIES that are found from all samples, are computed
#    sys bash $path/scripts/uniqSpecies.sh
# }      

# if  ( paramsOfTaxonomy{'gene'} == 'gene_COI' &&  paramsOfTaxonomy{'clusteringAlgoForCOI_ITS'} == 'algo_CROP' ){
#    print ' EDO EINAI POU XOUME TO KENAKI MAS TO TELEUTAIO.....' +"\n"
#    string cropPath = genePath + '/' + 'gene_COI' +  '/' + 'CROP'
#    print cropPath + "\n"
#    cropPath.chdir()
#    string[] allSamplesList
#    string[] allContainsOfCrop = cropPath.dir()
#    for ( string sample : allContainsOfCrop ) {
#       print "sample is " + sample + "\n"
#       if ( sample.endsWith('.finalTriplesFromCropClustering.txt') == true  ) {
#          string sampleId = sample.split('.finalTriplesFromCropClustering.txt')[0]
#          print "sample is " + sampleId + "\n"
#          sys mv $sample $outputPerSamplePath/$sampleId
#       }
#    }
   
#    # here, the number of the SPECIES that are found from all samples, are computed
#    #using string.readLines() i will create my otu table..  
#    #string.readLines() 
# }      

# string checkPhyloseq = outputPoint + "/phyloseq.chp"
# checkpoint checkPhyloseq 
# parameterFilePath.chdir()
# string m = "parameters.tsv"
# string{} paramForPhyloseq = readParameterFile(m)





# ###  make a file where i will move all the checkpoints
# parameterFilePath.chdir() ;
# if ( paramForPhyloseq{'emptyCheckpoints'} == 'Yes' ) {
#    string checkpointsDir = 'checkpoints_for_' + paramForPhyloseq{'outputFolderName'}
#    outputFilePath.chdir()
#    checkpointsDir.mkdir()
#    outputPoint.chdir()   
#    sys mv *.chp $outputFilePath/$checkpointsDir
# }

# # # keep only one OTU-table for every analysis in the case of the 16S marker gene and the alignment based taxonomy assignment method
# # if ( paramForPhyloseq{'gene'} == '16S' &&  paramForPhyloseq{'taxonomyAssignmentMethod'} == 'alignment' ) {
# # 
# #    string algoCluster = paramForPhyloseq{'clusteringAlgoFor16S_18SrRNA'}
# #    string algo = algoCluster.split('_')[1]   
# #    string assignmentPath = genePath + '/' + "gene_16S" + '/' + algo + '/' + paramForPhyloseq{'taxonomyFolderName'}
# #    assignmentPath.chdir()
# #    sys cd 
# #    sys mv 16S_otu_table.txt 16S_otutab_$paramForPhyloseq{'taxonomyFolderName'}.txt
# # }

# outputFilePath.chdir()
# print(" \n PEMA has come to an end! Let biology start! \n ")

# ###################################################################################################################
# #                                                                        
# #                              PEMA has been concluded.                              
# #                                                                        
# ###################################################################################################################
